{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team as an entity model\n",
    "\n",
    "This model is a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "from models.models import *\n",
    "from datasets.datasets import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meta\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = './Data/team/processed/team_data.xlsx'\n",
    "dataset = get_team_dataset(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63\n",
      "574\n",
      "(tensor([[0.6830, 2.9900, 2.6300],\n",
      "        [0.5490, 2.5500, 2.7200],\n",
      "        [0.5000, 2.5900, 2.9300],\n",
      "        [0.2930, 2.0100, 3.3700],\n",
      "        [0.5790, 3.1100, 2.8800]]), tensor([0.5490, 3.1500, 2.9800]))\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "test_length = len(dataset)//10\n",
    "train_length = len(dataset) - test_length\n",
    "\n",
    "print(test_length)\n",
    "print(train_length)\n",
    "dataset_test, dataset_train = random_split(dataset, [test_length, train_length])\n",
    "\n",
    "\n",
    "# Must generate batches of sequence data with the following format:\n",
    "# (batch_size, num_seasons(N), input_size(num stats))\n",
    "# (https://stackoverflow.com/questions/49466894/how-to-correctly-give-inputs-to-embedding-lstm-and-linear-layers-in-pytorch/49473068#49473068)\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(dataset_train.__getitem__(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = len(dataset_train.__getitem__(0)[0][0])\n",
    "hidden_size = 50\n",
    "#model = TeamAsEntity(input_size=input_size, hidden_size=hidden_size, device=device).to(device)\n",
    "\n",
    "\"\"\"\n",
    "Hidden Layers Size Exploration\n",
    "\"\"\"\n",
    "hidden_sizes = [50, 100, 200]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Stacked LSTM Exploration\n",
    "\"\"\"\n",
    "num_layerss = [1,2,3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = RMSELoss()\n",
    "opt = torch.optim.Adam\n",
    "epochs = 1500\n",
    "loss_interval = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "####################\n",
      "Model: 50_1\n",
      "####################\n",
      "\n",
      "Validation loss for epoch 0: [2.119821548461914]\n",
      "Validation loss for epoch 25: [1.0773494243621826]\n",
      "Validation loss for epoch 50: [0.8247585296630859]\n",
      "Validation loss for epoch 75: [0.6613538265228271]\n",
      "Validation loss for epoch 100: [0.5250433683395386]\n",
      "Validation loss for epoch 125: [0.4199964702129364]\n",
      "Validation loss for epoch 150: [0.35582515597343445]\n",
      "Validation loss for epoch 175: [0.32929426431655884]\n",
      "Validation loss for epoch 200: [0.32241377234458923]\n",
      "Validation loss for epoch 225: [0.3214876651763916]\n",
      "Validation loss for epoch 250: [0.32162684202194214]\n",
      "Validation loss for epoch 275: [0.32178211212158203]\n",
      "Validation loss for epoch 300: [0.3218347430229187]\n",
      "Validation loss for epoch 325: [0.32184943556785583]\n",
      "Validation loss for epoch 350: [0.321842223405838]\n",
      "Validation loss for epoch 375: [0.32184502482414246]\n",
      "Validation loss for epoch 400: [0.3218744695186615]\n",
      "Validation loss for epoch 425: [0.32185330986976624]\n",
      "Validation loss for epoch 450: [0.3218410909175873]\n",
      "Validation loss for epoch 475: [0.32182392477989197]\n",
      "Validation loss for epoch 500: [0.32184475660324097]\n",
      "Validation loss for epoch 525: [0.32185497879981995]\n",
      "Validation loss for epoch 550: [0.3218235671520233]\n",
      "Validation loss for epoch 575: [0.3218007981777191]\n",
      "Validation loss for epoch 600: [0.3217242658138275]\n",
      "Validation loss for epoch 625: [0.321675181388855]\n",
      "Validation loss for epoch 650: [0.32147058844566345]\n",
      "Validation loss for epoch 675: [0.32092607021331787]\n",
      "Validation loss for epoch 700: [0.3182239234447479]\n",
      "Validation loss for epoch 725: [0.3124164342880249]\n",
      "Validation loss for epoch 750: [0.302760511636734]\n",
      "Validation loss for epoch 775: [0.2835862338542938]\n",
      "Validation loss for epoch 800: [0.26500651240348816]\n",
      "Validation loss for epoch 825: [0.2576417922973633]\n",
      "Validation loss for epoch 850: [0.25393879413604736]\n",
      "Validation loss for epoch 875: [0.2519979476928711]\n",
      "Validation loss for epoch 900: [0.25044384598731995]\n",
      "Validation loss for epoch 925: [0.2506222128868103]\n",
      "Validation loss for epoch 950: [0.24882856011390686]\n",
      "Validation loss for epoch 975: [0.24820660054683685]\n",
      "Validation loss for epoch 1000: [0.24869126081466675]\n",
      "Validation loss for epoch 1025: [0.24749575555324554]\n",
      "Validation loss for epoch 1050: [0.24950149655342102]\n",
      "Validation loss for epoch 1075: [0.24765914678573608]\n",
      "Validation loss for epoch 1100: [0.24856159090995789]\n",
      "Validation loss for epoch 1125: [0.24751496315002441]\n",
      "Validation loss for epoch 1150: [0.24642114341259003]\n",
      "Validation loss for epoch 1175: [0.24708573520183563]\n",
      "Validation loss for epoch 1200: [0.2463301122188568]\n",
      "Validation loss for epoch 1225: [0.24771828949451447]\n",
      "Validation loss for epoch 1250: [0.2465217411518097]\n",
      "Validation loss for epoch 1275: [0.24687039852142334]\n",
      "Validation loss for epoch 1300: [0.24707768857479095]\n",
      "Validation loss for epoch 1325: [0.24674423038959503]\n",
      "Validation loss for epoch 1350: [0.24628621339797974]\n",
      "Validation loss for epoch 1375: [0.24711646139621735]\n",
      "Validation loss for epoch 1400: [0.24700681865215302]\n",
      "Validation loss for epoch 1425: [0.24740266799926758]\n",
      "Validation loss for epoch 1450: [0.24653005599975586]\n",
      "Validation loss for epoch 1475: [0.2456059753894806]\n",
      "\n",
      "####################\n",
      "Model: 50_2\n",
      "####################\n",
      "\n",
      "Validation loss for epoch 0: [2.394061326980591]\n",
      "Validation loss for epoch 25: [1.5494678020477295]\n",
      "Validation loss for epoch 50: [1.3349237442016602]\n",
      "Validation loss for epoch 75: [1.1731148958206177]\n",
      "Validation loss for epoch 100: [1.0274025201797485]\n",
      "Validation loss for epoch 125: [0.8937172889709473]\n",
      "Validation loss for epoch 150: [0.7715103626251221]\n",
      "Validation loss for epoch 175: [0.6612316370010376]\n",
      "Validation loss for epoch 200: [0.5636687874794006]\n",
      "Validation loss for epoch 225: [0.48053663969039917]\n",
      "Validation loss for epoch 250: [0.4145268201828003]\n",
      "Validation loss for epoch 275: [0.36823657155036926]\n",
      "Validation loss for epoch 300: [0.3413001000881195]\n",
      "Validation loss for epoch 325: [0.3285827338695526]\n",
      "Validation loss for epoch 350: [0.3237041234970093]\n",
      "Validation loss for epoch 375: [0.32214030623435974]\n",
      "Validation loss for epoch 400: [0.321783185005188]\n",
      "Validation loss for epoch 425: [0.32180941104888916]\n",
      "Validation loss for epoch 450: [0.32185128331184387]\n",
      "Validation loss for epoch 475: [0.32185283303260803]\n",
      "Validation loss for epoch 500: [0.3218904435634613]\n",
      "Validation loss for epoch 525: [0.3219029903411865]\n",
      "Validation loss for epoch 550: [0.32187074422836304]\n",
      "Validation loss for epoch 575: [0.3219277560710907]\n",
      "Validation loss for epoch 600: [0.3218722641468048]\n",
      "Validation loss for epoch 625: [0.321907639503479]\n",
      "Validation loss for epoch 650: [0.32185399532318115]\n",
      "Validation loss for epoch 675: [0.3218097984790802]\n",
      "Validation loss for epoch 700: [0.3215596675872803]\n",
      "Validation loss for epoch 725: [0.3211851716041565]\n",
      "Validation loss for epoch 750: [0.3193836808204651]\n",
      "Validation loss for epoch 775: [0.31127816438674927]\n",
      "Validation loss for epoch 800: [0.3070831000804901]\n",
      "Validation loss for epoch 825: [0.3052196502685547]\n",
      "Validation loss for epoch 850: [0.3025181293487549]\n",
      "Validation loss for epoch 875: [0.30138710141181946]\n",
      "Validation loss for epoch 900: [0.29991596937179565]\n",
      "Validation loss for epoch 925: [0.29859355092048645]\n",
      "Validation loss for epoch 950: [0.29744160175323486]\n",
      "Validation loss for epoch 975: [0.29659897089004517]\n",
      "Validation loss for epoch 1000: [0.2956956923007965]\n",
      "Validation loss for epoch 1025: [0.295030415058136]\n",
      "Validation loss for epoch 1050: [0.2956507205963135]\n",
      "Validation loss for epoch 1075: [0.2945460379123688]\n",
      "Validation loss for epoch 1100: [0.2944018244743347]\n",
      "Validation loss for epoch 1125: [0.29362577199935913]\n",
      "Validation loss for epoch 1150: [0.2933615446090698]\n",
      "Validation loss for epoch 1175: [0.29209062457084656]\n",
      "Validation loss for epoch 1200: [0.2893894612789154]\n",
      "Validation loss for epoch 1225: [0.27591410279273987]\n",
      "Validation loss for epoch 1250: [0.25814878940582275]\n",
      "Validation loss for epoch 1275: [0.2541052997112274]\n",
      "Validation loss for epoch 1300: [0.24987415969371796]\n",
      "Validation loss for epoch 1325: [0.24892117083072662]\n",
      "Validation loss for epoch 1350: [0.2482244372367859]\n",
      "Validation loss for epoch 1375: [0.24871937930583954]\n",
      "Validation loss for epoch 1400: [0.24779197573661804]\n",
      "Validation loss for epoch 1425: [0.2486785352230072]\n",
      "Validation loss for epoch 1450: [0.2479574978351593]\n",
      "Validation loss for epoch 1475: [0.24828480184078217]\n",
      "\n",
      "####################\n",
      "Model: 50_3\n",
      "####################\n",
      "\n",
      "Validation loss for epoch 0: [2.351309061050415]\n",
      "Validation loss for epoch 25: [1.2370338439941406]\n",
      "Validation loss for epoch 50: [1.0063749551773071]\n",
      "Validation loss for epoch 75: [0.8506979942321777]\n",
      "Validation loss for epoch 100: [0.7182327508926392]\n",
      "Validation loss for epoch 125: [0.6038065552711487]\n",
      "Validation loss for epoch 150: [0.507235050201416]\n",
      "Validation loss for epoch 175: [0.4307645261287689]\n",
      "Validation loss for epoch 200: [0.37557438015937805]\n",
      "Validation loss for epoch 225: [0.3426033556461334]\n",
      "Validation loss for epoch 250: [0.3274517357349396]\n",
      "Validation loss for epoch 275: [0.32270485162734985]\n",
      "Validation loss for epoch 300: [0.32170915603637695]\n",
      "Validation loss for epoch 325: [0.32174453139305115]\n",
      "Validation loss for epoch 350: [0.32184863090515137]\n",
      "Validation loss for epoch 375: [0.32188689708709717]\n",
      "Validation loss for epoch 400: [0.3219204545021057]\n",
      "Validation loss for epoch 425: [0.3219597041606903]\n",
      "Validation loss for epoch 450: [0.3219183385372162]\n",
      "Validation loss for epoch 475: [0.32190024852752686]\n",
      "Validation loss for epoch 500: [0.3219107687473297]\n",
      "Validation loss for epoch 525: [0.3218390643596649]\n",
      "Validation loss for epoch 550: [0.3219045400619507]\n",
      "Validation loss for epoch 575: [0.32190173864364624]\n",
      "Validation loss for epoch 600: [0.3219355046749115]\n",
      "Validation loss for epoch 625: [0.3219493627548218]\n",
      "Validation loss for epoch 650: [0.32185161113739014]\n",
      "Validation loss for epoch 675: [0.32187870144844055]\n",
      "Validation loss for epoch 700: [0.3219220042228699]\n",
      "Validation loss for epoch 725: [0.3219575583934784]\n",
      "Validation loss for epoch 750: [0.3218827247619629]\n",
      "Validation loss for epoch 775: [0.3219282031059265]\n",
      "Validation loss for epoch 800: [0.3218536078929901]\n",
      "Validation loss for epoch 825: [0.32192060351371765]\n",
      "Validation loss for epoch 850: [0.32191023230552673]\n",
      "Validation loss for epoch 875: [0.3219165802001953]\n",
      "Validation loss for epoch 900: [0.32185614109039307]\n",
      "Validation loss for epoch 925: [0.32189515233039856]\n",
      "Validation loss for epoch 950: [0.32198241353034973]\n",
      "Validation loss for epoch 975: [0.3219120502471924]\n",
      "Validation loss for epoch 1000: [0.32187241315841675]\n",
      "Validation loss for epoch 1025: [0.3218821585178375]\n",
      "Validation loss for epoch 1050: [0.3218114674091339]\n",
      "Validation loss for epoch 1075: [0.3184152841567993]\n",
      "Validation loss for epoch 1100: [0.30266860127449036]\n",
      "Validation loss for epoch 1125: [0.2971492409706116]\n",
      "Validation loss for epoch 1150: [0.29345282912254333]\n",
      "Validation loss for epoch 1175: [0.2922774851322174]\n",
      "Validation loss for epoch 1200: [0.2907160222530365]\n",
      "Validation loss for epoch 1225: [0.28387171030044556]\n",
      "Validation loss for epoch 1250: [0.265927255153656]\n",
      "Validation loss for epoch 1275: [0.2610711455345154]\n",
      "Validation loss for epoch 1300: [0.25971370935440063]\n",
      "Validation loss for epoch 1325: [0.2585145831108093]\n",
      "Validation loss for epoch 1350: [0.2574910521507263]\n",
      "Validation loss for epoch 1375: [0.2557399868965149]\n",
      "Validation loss for epoch 1400: [0.25736305117607117]\n",
      "Validation loss for epoch 1425: [0.25557541847229004]\n",
      "Validation loss for epoch 1450: [0.254159539937973]\n",
      "Validation loss for epoch 1475: [0.25446000695228577]\n",
      "\n",
      "####################\n",
      "Model: 100_1\n",
      "####################\n",
      "\n",
      "Validation loss for epoch 0: [2.387070655822754]\n",
      "Validation loss for epoch 25: [1.4218798875808716]\n",
      "Validation loss for epoch 50: [1.2075166702270508]\n",
      "Validation loss for epoch 75: [1.0218260288238525]\n",
      "Validation loss for epoch 100: [0.8522217869758606]\n",
      "Validation loss for epoch 125: [0.6982290148735046]\n",
      "Validation loss for epoch 150: [0.5626814961433411]\n",
      "Validation loss for epoch 175: [0.45195966958999634]\n",
      "Validation loss for epoch 200: [0.37575608491897583]\n",
      "Validation loss for epoch 225: [0.3373148739337921]\n",
      "Validation loss for epoch 250: [0.32440251111984253]\n",
      "Validation loss for epoch 275: [0.32167407870292664]\n",
      "Validation loss for epoch 300: [0.3215091824531555]\n",
      "Validation loss for epoch 325: [0.32169976830482483]\n",
      "Validation loss for epoch 350: [0.3218155801296234]\n",
      "Validation loss for epoch 375: [0.3218728005886078]\n",
      "Validation loss for epoch 400: [0.3218827247619629]\n",
      "Validation loss for epoch 425: [0.3219178020954132]\n",
      "Validation loss for epoch 450: [0.321909099817276]\n",
      "Validation loss for epoch 475: [0.32190626859664917]\n",
      "Validation loss for epoch 500: [0.321901798248291]\n",
      "Validation loss for epoch 525: [0.3218930959701538]\n",
      "Validation loss for epoch 550: [0.3219294250011444]\n",
      "Validation loss for epoch 575: [0.3218846023082733]\n",
      "Validation loss for epoch 600: [0.32189199328422546]\n",
      "Validation loss for epoch 625: [0.32188624143600464]\n",
      "Validation loss for epoch 650: [0.32182577252388]\n",
      "Validation loss for epoch 675: [0.3211413025856018]\n",
      "Validation loss for epoch 700: [0.3135526478290558]\n",
      "Validation loss for epoch 725: [0.2775973677635193]\n",
      "Validation loss for epoch 750: [0.26364758610725403]\n",
      "Validation loss for epoch 775: [0.2611364424228668]\n",
      "Validation loss for epoch 800: [0.2587841749191284]\n",
      "Validation loss for epoch 825: [0.25617071986198425]\n",
      "Validation loss for epoch 850: [0.25459590554237366]\n",
      "Validation loss for epoch 875: [0.25467342138290405]\n",
      "Validation loss for epoch 900: [0.252871572971344]\n",
      "Validation loss for epoch 925: [0.25213518738746643]\n",
      "Validation loss for epoch 950: [0.2510838806629181]\n",
      "Validation loss for epoch 975: [0.2508368492126465]\n",
      "Validation loss for epoch 1000: [0.24902214109897614]\n",
      "Validation loss for epoch 1025: [0.24871288239955902]\n",
      "Validation loss for epoch 1050: [0.24757687747478485]\n",
      "Validation loss for epoch 1075: [0.24814125895500183]\n",
      "Validation loss for epoch 1100: [0.24822036921977997]\n",
      "Validation loss for epoch 1125: [0.24827325344085693]\n",
      "Validation loss for epoch 1150: [0.2482215315103531]\n",
      "Validation loss for epoch 1175: [0.24851304292678833]\n",
      "Validation loss for epoch 1200: [0.24781404435634613]\n",
      "Validation loss for epoch 1225: [0.24759343266487122]\n",
      "Validation loss for epoch 1250: [0.24851460754871368]\n",
      "Validation loss for epoch 1275: [0.24584312736988068]\n",
      "Validation loss for epoch 1300: [0.24579715728759766]\n",
      "Validation loss for epoch 1325: [0.2467924803495407]\n",
      "Validation loss for epoch 1350: [0.24495717883110046]\n",
      "Validation loss for epoch 1375: [0.2456265091896057]\n",
      "Validation loss for epoch 1400: [0.2451646476984024]\n",
      "Validation loss for epoch 1425: [0.2452932745218277]\n",
      "Validation loss for epoch 1450: [0.24528902769088745]\n",
      "Validation loss for epoch 1475: [0.2458738088607788]\n",
      "\n",
      "####################\n",
      "Model: 100_2\n",
      "####################\n",
      "\n",
      "Validation loss for epoch 0: [2.510356903076172]\n",
      "Validation loss for epoch 25: [1.105918288230896]\n",
      "Validation loss for epoch 50: [0.9262478351593018]\n",
      "Validation loss for epoch 75: [0.7722648978233337]\n",
      "Validation loss for epoch 100: [0.6336652636528015]\n",
      "Validation loss for epoch 125: [0.5139107704162598]\n",
      "Validation loss for epoch 150: [0.4211499094963074]\n",
      "Validation loss for epoch 175: [0.3623099625110626]\n",
      "Validation loss for epoch 200: [0.3342911899089813]\n",
      "Validation loss for epoch 225: [0.32439884543418884]\n",
      "Validation loss for epoch 250: [0.3218644857406616]\n",
      "Validation loss for epoch 275: [0.3215537667274475]\n",
      "Validation loss for epoch 300: [0.32168710231781006]\n",
      "Validation loss for epoch 325: [0.32180356979370117]\n",
      "Validation loss for epoch 350: [0.3218752145767212]\n",
      "Validation loss for epoch 375: [0.3218799829483032]\n",
      "Validation loss for epoch 400: [0.3219160735607147]\n",
      "Validation loss for epoch 425: [0.32191070914268494]\n",
      "Validation loss for epoch 450: [0.32189449667930603]\n",
      "Validation loss for epoch 475: [0.32191041111946106]\n",
      "Validation loss for epoch 500: [0.32191067934036255]\n",
      "Validation loss for epoch 525: [0.32189011573791504]\n",
      "Validation loss for epoch 550: [0.3219047784805298]\n",
      "Validation loss for epoch 575: [0.32187721133232117]\n",
      "Validation loss for epoch 600: [0.3219049274921417]\n",
      "Validation loss for epoch 625: [0.321907639503479]\n",
      "Validation loss for epoch 650: [0.3218875229358673]\n",
      "Validation loss for epoch 675: [0.3218543231487274]\n",
      "Validation loss for epoch 700: [0.32190045714378357]\n",
      "Validation loss for epoch 725: [0.3218231499195099]\n",
      "Validation loss for epoch 750: [0.3212191164493561]\n",
      "Validation loss for epoch 775: [0.3151537775993347]\n",
      "Validation loss for epoch 800: [0.3017590641975403]\n",
      "Validation loss for epoch 825: [0.29087892174720764]\n",
      "Validation loss for epoch 850: [0.28983622789382935]\n",
      "Validation loss for epoch 875: [0.2774791121482849]\n",
      "Validation loss for epoch 900: [0.26825761795043945]\n",
      "Validation loss for epoch 925: [0.26403430104255676]\n",
      "Validation loss for epoch 950: [0.2584454417228699]\n",
      "Validation loss for epoch 975: [0.25527259707450867]\n",
      "Validation loss for epoch 1000: [0.2542217969894409]\n",
      "Validation loss for epoch 1025: [0.2506277859210968]\n",
      "Validation loss for epoch 1050: [0.25148454308509827]\n",
      "Validation loss for epoch 1075: [0.24916066229343414]\n",
      "Validation loss for epoch 1100: [0.24913200736045837]\n",
      "Validation loss for epoch 1125: [0.25011736154556274]\n",
      "Validation loss for epoch 1150: [0.24809454381465912]\n",
      "Validation loss for epoch 1175: [0.24870575964450836]\n",
      "Validation loss for epoch 1200: [0.24649013578891754]\n",
      "Validation loss for epoch 1225: [0.2474694550037384]\n",
      "Validation loss for epoch 1250: [0.246333047747612]\n",
      "Validation loss for epoch 1275: [0.24600349366664886]\n",
      "Validation loss for epoch 1300: [0.2458418309688568]\n",
      "Validation loss for epoch 1325: [0.24529895186424255]\n",
      "Validation loss for epoch 1350: [0.24819731712341309]\n",
      "Validation loss for epoch 1375: [0.24541860818862915]\n",
      "Validation loss for epoch 1400: [0.24508781731128693]\n",
      "Validation loss for epoch 1425: [0.24612785875797272]\n",
      "Validation loss for epoch 1450: [0.24554024636745453]\n",
      "Validation loss for epoch 1475: [0.24555601179599762]\n",
      "\n",
      "####################\n",
      "Model: 100_3\n",
      "####################\n",
      "\n",
      "Validation loss for epoch 0: [2.5573465824127197]\n",
      "Validation loss for epoch 25: [1.4236950874328613]\n",
      "Validation loss for epoch 50: [1.2219631671905518]\n",
      "Validation loss for epoch 75: [1.0463356971740723]\n",
      "Validation loss for epoch 100: [0.8877823352813721]\n",
      "Validation loss for epoch 125: [0.7402228713035583]\n",
      "Validation loss for epoch 150: [0.605495810508728]\n",
      "Validation loss for epoch 175: [0.48984819650650024]\n",
      "Validation loss for epoch 200: [0.4029170572757721]\n",
      "Validation loss for epoch 225: [0.35156452655792236]\n",
      "Validation loss for epoch 250: [0.32979902625083923]\n",
      "Validation loss for epoch 275: [0.3232106864452362]\n",
      "Validation loss for epoch 300: [0.3217998445034027]\n",
      "Validation loss for epoch 325: [0.3217031955718994]\n",
      "Validation loss for epoch 350: [0.3217965364456177]\n",
      "Validation loss for epoch 375: [0.32187554240226746]\n",
      "Validation loss for epoch 400: [0.3219045400619507]\n",
      "Validation loss for epoch 425: [0.3218972086906433]\n",
      "Validation loss for epoch 450: [0.32191893458366394]\n",
      "Validation loss for epoch 475: [0.32190757989883423]\n",
      "Validation loss for epoch 500: [0.3219369649887085]\n",
      "Validation loss for epoch 525: [0.32192355394363403]\n",
      "Validation loss for epoch 550: [0.3219238221645355]\n",
      "Validation loss for epoch 575: [0.3219413757324219]\n",
      "Validation loss for epoch 600: [0.32192176580429077]\n",
      "Validation loss for epoch 625: [0.3219217360019684]\n",
      "Validation loss for epoch 650: [0.32191237807273865]\n",
      "Validation loss for epoch 675: [0.32192981243133545]\n",
      "Validation loss for epoch 700: [0.3219298720359802]\n",
      "Validation loss for epoch 725: [0.321916401386261]\n",
      "Validation loss for epoch 750: [0.32189467549324036]\n",
      "Validation loss for epoch 775: [0.32189562916755676]\n",
      "Validation loss for epoch 800: [0.32191547751426697]\n",
      "Validation loss for epoch 825: [0.3218967020511627]\n",
      "Validation loss for epoch 850: [0.3219110667705536]\n",
      "Validation loss for epoch 875: [0.3218681812286377]\n",
      "Validation loss for epoch 900: [0.321892648935318]\n",
      "Validation loss for epoch 925: [0.3219149708747864]\n",
      "Validation loss for epoch 950: [0.32184553146362305]\n",
      "Validation loss for epoch 975: [0.3206478953361511]\n",
      "Validation loss for epoch 1000: [0.3129764795303345]\n",
      "Validation loss for epoch 1025: [0.30860310792922974]\n",
      "Validation loss for epoch 1050: [0.29117557406425476]\n",
      "Validation loss for epoch 1075: [0.2746247947216034]\n",
      "Validation loss for epoch 1100: [0.2680373787879944]\n",
      "Validation loss for epoch 1125: [0.26305222511291504]\n",
      "Validation loss for epoch 1150: [0.25472134351730347]\n",
      "Validation loss for epoch 1175: [0.25197064876556396]\n",
      "Validation loss for epoch 1200: [0.2537088096141815]\n",
      "Validation loss for epoch 1225: [0.2482767105102539]\n",
      "Validation loss for epoch 1250: [0.24775423109531403]\n",
      "Validation loss for epoch 1275: [0.248880997300148]\n",
      "Validation loss for epoch 1300: [0.24424390494823456]\n",
      "Validation loss for epoch 1325: [0.2442304641008377]\n",
      "Validation loss for epoch 1350: [0.2429521679878235]\n",
      "Validation loss for epoch 1375: [0.24456462264060974]\n",
      "Validation loss for epoch 1400: [0.2425997257232666]\n",
      "Validation loss for epoch 1425: [0.24219577014446259]\n",
      "Validation loss for epoch 1450: [0.2451418936252594]\n",
      "Validation loss for epoch 1475: [0.24299873411655426]\n",
      "\n",
      "####################\n",
      "Model: 200_1\n",
      "####################\n",
      "\n",
      "Validation loss for epoch 0: [2.2430405616760254]\n",
      "Validation loss for epoch 25: [1.3535703420639038]\n",
      "Validation loss for epoch 50: [1.1566722393035889]\n",
      "Validation loss for epoch 75: [0.9767699837684631]\n",
      "Validation loss for epoch 100: [0.8103705048561096]\n",
      "Validation loss for epoch 125: [0.6595654487609863]\n",
      "Validation loss for epoch 150: [0.5291175246238708]\n",
      "Validation loss for epoch 175: [0.42688632011413574]\n",
      "Validation loss for epoch 200: [0.3616304397583008]\n",
      "Validation loss for epoch 225: [0.3320727050304413]\n",
      "Validation loss for epoch 250: [0.3232029378414154]\n",
      "Validation loss for epoch 275: [0.321593701839447]\n",
      "Validation loss for epoch 300: [0.3216075301170349]\n",
      "Validation loss for epoch 325: [0.32176539301872253]\n",
      "Validation loss for epoch 350: [0.321860671043396]\n",
      "Validation loss for epoch 375: [0.3219034969806671]\n",
      "Validation loss for epoch 400: [0.32190385460853577]\n",
      "Validation loss for epoch 425: [0.3218909204006195]\n",
      "Validation loss for epoch 450: [0.321902871131897]\n",
      "Validation loss for epoch 475: [0.3219004273414612]\n",
      "Validation loss for epoch 500: [0.32188737392425537]\n",
      "Validation loss for epoch 525: [0.32191377878189087]\n",
      "Validation loss for epoch 550: [0.321883887052536]\n",
      "Validation loss for epoch 575: [0.32154712080955505]\n",
      "Validation loss for epoch 600: [0.31364789605140686]\n",
      "Validation loss for epoch 625: [0.2977273762226105]\n",
      "Validation loss for epoch 650: [0.27620425820350647]\n",
      "Validation loss for epoch 675: [0.2705763578414917]\n",
      "Validation loss for epoch 700: [0.2666015923023224]\n",
      "Validation loss for epoch 725: [0.25968533754348755]\n",
      "Validation loss for epoch 750: [0.2562564015388489]\n",
      "Validation loss for epoch 775: [0.25374284386634827]\n",
      "Validation loss for epoch 800: [0.25198572874069214]\n",
      "Validation loss for epoch 825: [0.25068777799606323]\n",
      "Validation loss for epoch 850: [0.2490307092666626]\n",
      "Validation loss for epoch 875: [0.2484910488128662]\n",
      "Validation loss for epoch 900: [0.24907152354717255]\n",
      "Validation loss for epoch 925: [0.24780943989753723]\n",
      "Validation loss for epoch 950: [0.2470695823431015]\n",
      "Validation loss for epoch 975: [0.2467455416917801]\n",
      "Validation loss for epoch 1000: [0.2462051957845688]\n",
      "Validation loss for epoch 1025: [0.2450847029685974]\n",
      "Validation loss for epoch 1050: [0.2458057850599289]\n",
      "Validation loss for epoch 1075: [0.24509505927562714]\n",
      "Validation loss for epoch 1100: [0.24467706680297852]\n",
      "Validation loss for epoch 1125: [0.24475042521953583]\n",
      "Validation loss for epoch 1150: [0.24421319365501404]\n",
      "Validation loss for epoch 1175: [0.24698302149772644]\n",
      "Validation loss for epoch 1200: [0.2436663806438446]\n",
      "Validation loss for epoch 1225: [0.2449737936258316]\n",
      "Validation loss for epoch 1250: [0.24400454759597778]\n",
      "Validation loss for epoch 1275: [0.24544966220855713]\n",
      "Validation loss for epoch 1300: [0.24374519288539886]\n",
      "Validation loss for epoch 1325: [0.24323983490467072]\n",
      "Validation loss for epoch 1350: [0.24495792388916016]\n",
      "Validation loss for epoch 1375: [0.2447054088115692]\n",
      "Validation loss for epoch 1400: [0.24296605587005615]\n",
      "Validation loss for epoch 1425: [0.24594686925411224]\n",
      "Validation loss for epoch 1450: [0.2524820566177368]\n",
      "Validation loss for epoch 1475: [0.24534864723682404]\n",
      "\n",
      "####################\n",
      "Model: 200_2\n",
      "####################\n",
      "\n",
      "Validation loss for epoch 0: [2.347074508666992]\n",
      "Validation loss for epoch 25: [0.9807623624801636]\n",
      "Validation loss for epoch 50: [0.7977354526519775]\n",
      "Validation loss for epoch 75: [0.6408330798149109]\n",
      "Validation loss for epoch 100: [0.5099598169326782]\n",
      "Validation loss for epoch 125: [0.4114640951156616]\n",
      "Validation loss for epoch 150: [0.35262930393218994]\n",
      "Validation loss for epoch 175: [0.32856693863868713]\n",
      "Validation loss for epoch 200: [0.3222850263118744]\n",
      "Validation loss for epoch 225: [0.3214643895626068]\n",
      "Validation loss for epoch 250: [0.32162579894065857]\n",
      "Validation loss for epoch 275: [0.3217886984348297]\n",
      "Validation loss for epoch 300: [0.32188159227371216]\n",
      "Validation loss for epoch 325: [0.32192495465278625]\n",
      "Validation loss for epoch 350: [0.3219223916530609]\n",
      "Validation loss for epoch 375: [0.3219165802001953]\n",
      "Validation loss for epoch 400: [0.32190388441085815]\n",
      "Validation loss for epoch 425: [0.32189634442329407]\n",
      "Validation loss for epoch 450: [0.32191404700279236]\n",
      "Validation loss for epoch 475: [0.32189202308654785]\n",
      "Validation loss for epoch 500: [0.3218890130519867]\n",
      "Validation loss for epoch 525: [0.3218792676925659]\n",
      "Validation loss for epoch 550: [0.32167133688926697]\n",
      "Validation loss for epoch 575: [0.31848832964897156]\n",
      "Validation loss for epoch 600: [0.2965519428253174]\n",
      "Validation loss for epoch 625: [0.2765473425388336]\n",
      "Validation loss for epoch 650: [0.2688197195529938]\n",
      "Validation loss for epoch 675: [0.26378729939460754]\n",
      "Validation loss for epoch 700: [0.25789687037467957]\n",
      "Validation loss for epoch 725: [0.25543221831321716]\n",
      "Validation loss for epoch 750: [0.2614555060863495]\n",
      "Validation loss for epoch 775: [0.25086233019828796]\n",
      "Validation loss for epoch 800: [0.24834156036376953]\n",
      "Validation loss for epoch 825: [0.2468286007642746]\n",
      "Validation loss for epoch 850: [0.24607238173484802]\n",
      "Validation loss for epoch 875: [0.250557005405426]\n",
      "Validation loss for epoch 900: [0.24350574612617493]\n",
      "Validation loss for epoch 925: [0.24833182990550995]\n",
      "Validation loss for epoch 950: [0.2427573800086975]\n",
      "Validation loss for epoch 975: [0.2418859750032425]\n",
      "Validation loss for epoch 1000: [0.242060586810112]\n",
      "Validation loss for epoch 1025: [0.24102221429347992]\n",
      "Validation loss for epoch 1050: [0.24031391739845276]\n",
      "Validation loss for epoch 1075: [0.24045613408088684]\n",
      "Validation loss for epoch 1100: [0.24141359329223633]\n",
      "Validation loss for epoch 1125: [0.23986753821372986]\n",
      "Validation loss for epoch 1150: [0.23999880254268646]\n",
      "Validation loss for epoch 1175: [0.23974929749965668]\n",
      "Validation loss for epoch 1200: [0.24150943756103516]\n",
      "Validation loss for epoch 1225: [0.24553395807743073]\n",
      "Validation loss for epoch 1250: [0.23875577747821808]\n",
      "Validation loss for epoch 1275: [0.24021267890930176]\n",
      "Validation loss for epoch 1300: [0.2423630654811859]\n",
      "Validation loss for epoch 1325: [0.2440643310546875]\n",
      "Validation loss for epoch 1350: [0.23904180526733398]\n",
      "Validation loss for epoch 1375: [0.24036358296871185]\n",
      "Validation loss for epoch 1400: [0.24527877569198608]\n",
      "Validation loss for epoch 1425: [0.24177677929401398]\n",
      "Validation loss for epoch 1450: [0.23757457733154297]\n",
      "Validation loss for epoch 1475: [0.23867978155612946]\n",
      "\n",
      "####################\n",
      "Model: 200_3\n",
      "####################\n",
      "\n",
      "Validation loss for epoch 0: [2.3138697147369385]\n",
      "Validation loss for epoch 25: [1.2969986200332642]\n",
      "Validation loss for epoch 50: [1.128163456916809]\n",
      "Validation loss for epoch 75: [0.9727896451950073]\n",
      "Validation loss for epoch 100: [0.8256328701972961]\n",
      "Validation loss for epoch 125: [0.6880338191986084]\n",
      "Validation loss for epoch 150: [0.5642192363739014]\n",
      "Validation loss for epoch 175: [0.4613001346588135]\n",
      "Validation loss for epoch 200: [0.3876746892929077]\n",
      "Validation loss for epoch 225: [0.34619128704071045]\n",
      "Validation loss for epoch 250: [0.3285444378852844]\n",
      "Validation loss for epoch 275: [0.3229047358036041]\n",
      "Validation loss for epoch 300: [0.3216556906700134]\n",
      "Validation loss for epoch 325: [0.32158201932907104]\n",
      "Validation loss for epoch 350: [0.3217146396636963]\n",
      "Validation loss for epoch 375: [0.3218187093734741]\n",
      "Validation loss for epoch 400: [0.3218808174133301]\n",
      "Validation loss for epoch 425: [0.32189086079597473]\n",
      "Validation loss for epoch 450: [0.32188302278518677]\n",
      "Validation loss for epoch 475: [0.32190990447998047]\n",
      "Validation loss for epoch 500: [0.3219117820262909]\n",
      "Validation loss for epoch 525: [0.32191702723503113]\n",
      "Validation loss for epoch 550: [0.32191362977027893]\n",
      "Validation loss for epoch 575: [0.32193800806999207]\n",
      "Validation loss for epoch 600: [0.32193830609321594]\n",
      "Validation loss for epoch 625: [0.32189884781837463]\n",
      "Validation loss for epoch 650: [0.3218844532966614]\n",
      "Validation loss for epoch 675: [0.32192501425743103]\n",
      "Validation loss for epoch 700: [0.3218860924243927]\n",
      "Validation loss for epoch 725: [0.32191962003707886]\n",
      "Validation loss for epoch 750: [0.32193663716316223]\n",
      "Validation loss for epoch 775: [0.3219088017940521]\n",
      "Validation loss for epoch 800: [0.32117944955825806]\n",
      "Validation loss for epoch 825: [0.31073686480522156]\n",
      "Validation loss for epoch 850: [0.29564037919044495]\n",
      "Validation loss for epoch 875: [0.28933611512184143]\n",
      "Validation loss for epoch 900: [0.2884823977947235]\n",
      "Validation loss for epoch 925: [0.2806580662727356]\n",
      "Validation loss for epoch 950: [0.27788493037223816]\n",
      "Validation loss for epoch 975: [0.28066471219062805]\n",
      "Validation loss for epoch 1000: [0.27638691663742065]\n",
      "Validation loss for epoch 1025: [0.25836828351020813]\n",
      "Validation loss for epoch 1050: [0.2508957087993622]\n",
      "Validation loss for epoch 1075: [0.24636952579021454]\n",
      "Validation loss for epoch 1100: [0.24487647414207458]\n",
      "Validation loss for epoch 1125: [0.24199943244457245]\n",
      "Validation loss for epoch 1150: [0.24141012132167816]\n",
      "Validation loss for epoch 1175: [0.23824046552181244]\n",
      "Validation loss for epoch 1200: [0.24517717957496643]\n",
      "Validation loss for epoch 1225: [0.23786035180091858]\n",
      "Validation loss for epoch 1250: [0.23826345801353455]\n",
      "Validation loss for epoch 1275: [0.23699016869068146]\n",
      "Validation loss for epoch 1300: [0.2379142940044403]\n",
      "Validation loss for epoch 1325: [0.23814742267131805]\n",
      "Validation loss for epoch 1350: [0.23511359095573425]\n",
      "Validation loss for epoch 1375: [0.23952297866344452]\n",
      "Validation loss for epoch 1400: [0.24117092788219452]\n",
      "Validation loss for epoch 1425: [0.23583759367465973]\n",
      "Validation loss for epoch 1450: [0.23628245294094086]\n",
      "Validation loss for epoch 1475: [0.24002204835414886]\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for hidden_size in hidden_sizes:\n",
    "    for num_layers in num_layerss:\n",
    "\n",
    "        print(\"\\n####################\")\n",
    "        print(f\"Model: {hidden_size}_{num_layers}\")\n",
    "        print(\"####################\\n\")\n",
    "\n",
    "        model = TeamAsEntity(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, device=device).to(device)\n",
    "        optimizer = opt(model.parameters(),lr=0.0001)\n",
    "        \n",
    "        res = train_and_validate(model, criterion, optimizer, dataloader_train, dataloader_test, epochs=epochs, loss_interval=loss_interval, device=device)\n",
    "        results.append([(hidden_size,num_layers), res])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(50, 1), (TeamAsEntity(\n",
      "  (lstm): LSTM(3, 50, batch_first=True)\n",
      "  (mid): Sequential(\n",
      "    (0): Linear(in_features=50, out_features=200, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=200, out_features=12, bias=True)\n",
      "    (3): Tanh()\n",
      "  )\n",
      "  (out): Linear(in_features=12, out_features=3, bias=True)\n",
      "), [2.11114501953125, 1.1248048543930054, 0.867358922958374, 0.6410223245620728, 0.5164850950241089, 0.4303016662597656, 0.36312007904052734, 0.32792672514915466, 0.35896220803260803, 0.3184824287891388, 0.2976670265197754, 0.3006931245326996, 0.3059071898460388, 0.3176144063472748, 0.3333725929260254, 0.30416008830070496, 0.3156452775001526, 0.27824440598487854, 0.30548742413520813, 0.333980530500412, 0.3349428176879883, 0.30561092495918274, 0.333761602640152, 0.287723183631897, 0.3217295706272125, 0.3146878480911255, 0.31789061427116394, 0.3078920245170593, 0.3179381787776947, 0.31550684571266174, 0.3037567138671875, 0.32165035605430603, 0.2832488715648651, 0.2788577079772949, 0.25590038299560547, 0.24545696377754211, 0.26427721977233887, 0.25362375378608704, 0.25624194741249084, 0.2392129898071289, 0.2628830671310425, 0.23534716665744781, 0.24876199662685394, 0.24720631539821625, 0.2507461905479431, 0.24811214208602905, 0.25704309344291687, 0.2595199942588806, 0.23866546154022217, 0.2441433221101761, 0.22482554614543915, 0.2527766227722168, 0.2558801472187042, 0.23061135411262512, 0.2518334984779358, 0.2548409700393677, 0.22726339101791382, 0.24483071267604828, 0.27598071098327637, 0.2497165948152542], [2.119821548461914, 1.0773494243621826, 0.8247585296630859, 0.6613538265228271, 0.5250433683395386, 0.4199964702129364, 0.35582515597343445, 0.32929426431655884, 0.32241377234458923, 0.3214876651763916, 0.32162684202194214, 0.32178211212158203, 0.3218347430229187, 0.32184943556785583, 0.321842223405838, 0.32184502482414246, 0.3218744695186615, 0.32185330986976624, 0.3218410909175873, 0.32182392477989197, 0.32184475660324097, 0.32185497879981995, 0.3218235671520233, 0.3218007981777191, 0.3217242658138275, 0.321675181388855, 0.32147058844566345, 0.32092607021331787, 0.3182239234447479, 0.3124164342880249, 0.302760511636734, 0.2835862338542938, 0.26500651240348816, 0.2576417922973633, 0.25393879413604736, 0.2519979476928711, 0.25044384598731995, 0.2506222128868103, 0.24882856011390686, 0.24820660054683685, 0.24869126081466675, 0.24749575555324554, 0.24950149655342102, 0.24765914678573608, 0.24856159090995789, 0.24751496315002441, 0.24642114341259003, 0.24708573520183563, 0.2463301122188568, 0.24771828949451447, 0.2465217411518097, 0.24687039852142334, 0.24707768857479095, 0.24674423038959503, 0.24628621339797974, 0.24711646139621735, 0.24700681865215302, 0.24740266799926758, 0.24653005599975586, 0.2456059753894806])]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (60,) and (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m  results:\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(res)\n\u001b[1;32m----> 3\u001b[0m     \u001b[43mplot_train_v_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mteam-as-entity-\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mres\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m-\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mres\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mres\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mres\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_interval\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Jacoby\\Desktop\\Github\\hockey-player-stats-predictor\\utils.py:116\u001b[0m, in \u001b[0;36mplot_train_v_loss\u001b[1;34m(name, train_losses, val_losses, loss_epoch_step_size)\u001b[0m\n\u001b[0;32m    112\u001b[0m axs \u001b[38;5;241m=\u001b[39m (axs, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    114\u001b[0m xticks \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(val_losses)\u001b[38;5;241m*\u001b[39mloss_epoch_step_size, step\u001b[38;5;241m=\u001b[39mloss_epoch_step_size)\n\u001b[1;32m--> 116\u001b[0m \u001b[43maxs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxticks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_losses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmarker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mo\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43morange\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m axs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mset_title(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain/Val Loss\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    119\u001b[0m axs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mset_ylabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Jacoby\\.pyenv\\pyenv-win\\versions\\3.10.0\\lib\\site-packages\\matplotlib\\axes\\_axes.py:1688\u001b[0m, in \u001b[0;36mAxes.plot\u001b[1;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1445\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1446\u001b[0m \u001b[38;5;124;03mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[0;32m   1447\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1685\u001b[0m \u001b[38;5;124;03m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[0;32m   1686\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1687\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m cbook\u001b[38;5;241m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[38;5;241m.\u001b[39mLine2D)\n\u001b[1;32m-> 1688\u001b[0m lines \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_lines(\u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39mdata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)]\n\u001b[0;32m   1689\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n\u001b[0;32m   1690\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_line(line)\n",
      "File \u001b[1;32mc:\\Users\\Jacoby\\.pyenv\\pyenv-win\\versions\\3.10.0\\lib\\site-packages\\matplotlib\\axes\\_base.py:311\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[1;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m     this \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    310\u001b[0m     args \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m--> 311\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_plot_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Jacoby\\.pyenv\\pyenv-win\\versions\\3.10.0\\lib\\site-packages\\matplotlib\\axes\\_base.py:504\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[1;34m(self, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[0;32m    501\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes\u001b[38;5;241m.\u001b[39myaxis\u001b[38;5;241m.\u001b[39mupdate_units(y)\n\u001b[0;32m    503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n\u001b[1;32m--> 504\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y must have same first dimension, but \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    505\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhave shapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m    507\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y can be no greater than 2D, but have \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    508\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (60,) and (1,)"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABNYAAAG5CAYAAABC77mXAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAtG0lEQVR4nO3df5BV5WH/8c8isuDCLsQQMf5ASVFREZpRxGJAo7IEx2RS64CmUaYqik1NldoG04RoUojWTGJMYoyMhInJiAlq6oyw2PKjgYiFVjOZRhFREBUiKtwVRxaB8/3DYb8Sfsg9LCuF12vm/rHnnPvc5zjzZJd3zjm3piiKIgAAAABAVTp82BMAAAAAgP+LhDUAAAAAKEFYAwAAAIAShDUAAAAAKEFYAwAAAIAShDUAAAAAKEFYAwAAAIAShDUAAAAAKEFYAwAAAIAShDUAAAAAKKHqsHb//ffnmmuuyemnn57a2trU1NTkpz/9adUfvHXr1tx1113p379/unTpkp49e+bSSy/NCy+8UPVYAAAAANDeqg5r//zP/5yf/OQnWblyZY488sjSH3zNNdfk+uuvT1EUuf766zNixIg89NBDOeOMM7Js2bLS4wIAAABAe6g6rE2ZMiUrVqzI2rVrc+2115b60Llz52bKlCkZOnRo/ud//ie33XZbfvazn+WRRx7Jm2++mS996UulxgUAAACA9tKx2jecf/75e/2h9957b5Lkm9/8Zjp16tS6/TOf+UzOOeeczJ49Oy+99FKOPfbYvf4sAAAAANgXPpQvL5g3b17q6uoyZMiQHfY1NjYmSebPn9/e0wIAAACAPVb1FWt76+23387q1atz6qmn5pBDDtlhf9++fZPkA5+z1tLSkpaWltaft27dmjfffDOHH354ampq2nbSAAAAAPyfURRF3nrrrXz84x9Phw777rqydg9rlUolSdLQ0LDT/fX19dsdtyuTJ0/OLbfc0raTAwAAAOCAsWrVqhx99NH7bPx2D2ttZcKECbnxxhtbf65UKjn22GOzatWq1jgHAAAAwMGnubk5xxxzTLp167ZPP6fdw9q2K9V2dUVac3PzdsftSm1tbWpra3fYXl9fL6wBAAAAsM8fF9buX15QV1eXI488Mi+++GK2bNmyw/5tz1bb9qw1AAAAANgffSjfCjps2LC8/fbbWbhw4Q77mpqakiRDhw5t72kBAAAAwB7bp2Ht9ddfz7PPPpvXX399u+1jx45Nknzta1/Lpk2bWrfPnDkz8+bNy/Dhw9O7d+99OTUAAAAA2CtVP2NtypQpWbBgQZLk97//feu2efPmJUnOPvvsXHXVVUmSH/zgB7nlllsyceLEfOMb32gd49xzz81VV12VKVOm5JOf/GQuvPDCrF69OtOnT89HPvKR3HXXXXt5WgAAAACwb1Ud1hYsWJBp06Ztt23hwoXb3da5Laztzj333JP+/fvnJz/5Se6888507do1n//85/Mv//Iv+cQnPlHttAAAAACgXdUURVF82JNoC83NzWloaEilUvGtoAAAAAAHsfbqRB/KlxcAAAAAwP91whoAAAAAlCCsAQAAAEAJwhoAAAAAlCCsAQAAAEAJwhoAAAAAlCCsAQAAAEAJwhoAAAAAlCCsAQAAAEAJwhoAAAAAlCCsAQAAAEAJwhoAAAAAlCCsAQAAAEAJwhoAAAAAlCCsAQAAAEAJwhoAAAAAlCCsAQAAAEAJwhoAAAAAlCCsAQAAAEAJwhoAAAAAlCCsAQAAAEAJwhoAAAAAlCCsAQAAAEAJwhoAAAAAlCCsAQAAAEAJwhoAAAAAlCCsAQAAAEAJwhoAAAAAlCCsAQAAAEAJwhoAAAAAlCCsAQAAAEAJwhoAAAAAlCCsAQAAAEAJwhoAAAAAlCCsAQAAAEAJwhoAAAAAlCCsAQAAAEAJwhoAAAAAlCCsAQAAAEAJwhoAAAAAlCCsAQAAAEAJwhoAAAAAlCCsAQAAAEAJwhoAAAAAlCCsAQAAAEAJwhoAAAAAlCCsAQAAAEAJwhoAAAAAlCCsAQAAAEAJwhoAAAAAlCCsAQAAAEAJwhoAAAAAlCCsAQAAAEAJwhoAAAAAlCCsAQAAAEAJwhoAAAAAlCCsAQAAAEAJwhoAAAAAlCCsAQAAAEAJwhoAAAAAlCCsAQAAAEAJwhoAAAAAlFAqrC1evDgjR45M9+7dU1dXl8GDB+fBBx+saoxXX301X/7yl3PyySenrq4uRxxxRM4+++z87Gc/y5YtW8pMCwAAAADaTcdq3zB37tw0Njamc+fOGT16dLp165YZM2Zk1KhRWbVqVcaPH/+BY7zwwgs588wz88Ybb6SxsTEXXXRRmpub88gjj+Tyyy/PnDlzMnXq1FInBAAAAADtoaYoimJPD968eXNOOumkvPzyy1m0aFEGDhyYJKlUKhk0aFBWrFiR5557Lr17997tONddd13uvvvufO9738uXv/zl1u3r16/PgAED8tJLL2XFihUfOM77NTc3p6GhIZVKJfX19Xv8PgAAAAAOLO3Viaq6FXTOnDlZvnx5LrvsstaoliQNDQ25+eabs2nTpkybNu0Dx3nhhReSJCNHjtxue/fu3XP22WcnSV5//fVqpgYAAAAA7aqqsDZv3rwkyfDhw3fY19jYmCSZP3/+B45z6qmnJkkee+yx7bavX78+CxcuTK9evXLyySdXMzUAAAAAaFdVPWNt2bJlSZK+ffvusK9Xr17p2rVr6zG7c9NNN+XRRx/NDTfckFmzZuW0005rfcbaYYcdlocffjhdunTZ7RgtLS1paWlp/bm5ubmaUwEAAACAvVJVWKtUKkneu/VzZ+rr61uP2Z0jjjgiTzzxRP76r/86M2fOzKxZs5IkXbp0ybXXXpsBAwZ84BiTJ0/OLbfcUsXsAQAAAKDtVHUraFt5/vnnM2TIkKxduza/+c1v8tZbb2XVqlX5+te/nm9+85s577zzsmXLlt2OMWHChFQqldbXqlWr2mn2AAAAAFDlFWvbrlTb1VVpzc3N6dGjxweOM2bMmKxcuTIvvPBCevXqlSTp2rVrvvKVr+SPf/xjvve97+WBBx7IF77whV2OUVtbm9ra2mqmDwAAAABtpqor1rY9W21nz1Fbs2ZNNmzYsNPnr73fW2+9lYULF6Zfv36tUe39zj333CTJU089Vc3UAAAAAKBdVRXWhg0bliSZPXv2Dvuampq2O2ZXNm3alCR5/fXXd7p/7dq1SeJqNAAAAAD2a1WFtfPOOy99+vTJL37xizz99NOt2yuVSiZNmpROnTrl8ssvb92+evXqPPvss9vdOnr44YfnxBNPzEsvvZQpU6ZsN/769etzxx13JPn/V64BAAAAwP6oqrDWsWPHTJkyJVu3bs3QoUMzduzYjB8/PgMGDMhzzz2XSZMm5bjjjms9fsKECenXr18efvjh7cb57ne/m44dO+bqq6/O+eefn5tuuilXXXVVTjjhhDz77LO5+OKLc/7557fJCQIAAADAvlDVlxck711JtmDBgkycODHTp0/Pu+++m/79++e2227LqFGj9miMz3zmM/ntb3+bf/3Xf82CBQsyf/78dO7cOf369cvXv/71jBs3ruoTAQAAAID2VFMURfFhT6ItNDc3p6GhIZVKJfX19R/2dAAAAAD4kLRXJ6rqVlAAAAAA4D3CGgAAAACUIKwBAAAAQAnCGgAAAACUIKwBAAAAQAnCGgAAAACUIKwBAAAAQAnCGgAAAACUIKwBAAAAQAnCGgAAAACUIKwBAAAAQAnCGgAAAACUIKwBAAAAQAnCGgAAAACUIKwBAAAAQAnCGgAAAACUIKwBAAAAQAnCGgAAAACUIKwBAAAAQAnCGgAAAACUIKwBAAAAQAnCGgAAAACUIKwBAAAAQAnCGgAAAACUIKwBAAAAQAnCGgAAAACUIKwBAAAAQAnCGgAAAACUIKwBAAAAQAnCGgAAAACUIKwBAAAAQAnCGgAAAACUIKwBAAAAQAnCGgAAAACUIKwBAAAAQAnCGgAAAACUIKwBAAAAQAnCGgAAAACUIKwBAAAAQAnCGgAAAACUIKwBAAAAQAnCGgAAAACUIKwBAAAAQAnCGgAAAACUIKwBAAAAQAnCGgAAAACUIKwBAAAAQAnCGgAAAACUIKwBAAAAQAnCGgAAAACUIKwBAAAAQAnCGgAAAACUIKwBAAAAQAnCGgAAAACUIKwBAAAAQAnCGgAAAACUIKwBAAAAQAnCGgAAAACUIKwBAAAAQAnCGgAAAACUIKwBAAAAQAnCGgAAAACUUCqsLV68OCNHjkz37t1TV1eXwYMH58EHH6x6nNdeey033HBD+vbtm86dO+fwww/PWWedlbvvvrvMtAAAAACg3XSs9g1z585NY2NjOnfunNGjR6dbt26ZMWNGRo0alVWrVmX8+PF7NM7TTz+d4cOHZ926dbnwwgvzV3/1V9mwYUOeeeaZPProoxk3blzVJwMAAAAA7aWmKIpiTw/evHlzTjrppLz88stZtGhRBg4cmCSpVCoZNGhQVqxYkeeeey69e/fe7TjNzc3p379/3nnnnfz7v/97TjvttB0+p2PH6ppfc3NzGhoaUqlUUl9fX9V7AQAAADhwtFcnqupW0Dlz5mT58uW57LLLWqNakjQ0NOTmm2/Opk2bMm3atA8c50c/+lFeeumlfPvb394hqiWpOqoBAAAAQHurqmDNmzcvSTJ8+PAd9jU2NiZJ5s+f/4HjTJ8+PTU1Nbn44ouzdOnSzJ49O++8805OOumkjBgxIp06dapmWgAAAADQ7qoKa8uWLUuS9O3bd4d9vXr1SteuXVuP2ZVNmzbl97//fXr27Jm77rorEydOzNatW1v39+nTJ4888kj69++/23FaWlrS0tLS+nNzc3M1pwIAAAAAe6WqW0ErlUqS92793Jn6+vrWY3blzTffzJYtW/LGG2/k1ltvze23354//vGPefnll/O1r30tL774Yi666KJs3Lhxt+NMnjw5DQ0Nra9jjjmmmlMBAAAAgL1SVVhrC9uuTtuyZUuuu+66jB8/Ph/72Mdy1FFH5dZbb80ll1ySlStX5le/+tVux5kwYUIqlUrra9WqVe0xfQAAAABIUmVY23al2q6uStv2jQt7MkaSfPazn91h/7ZtS5Ys2e04tbW1qa+v3+4FAAAAAO2lqrC27dlqO3uO2po1a7Jhw4adPn/t/erq6nLUUUclSbp3777D/m3b3nnnnWqmBgAAAADtqqqwNmzYsCTJ7Nmzd9jX1NS03TG78+lPfzpJ8oc//GGHfdu2HXfccdVMDQAAAADaVU1RFMWeHrx58+aceOKJeeWVV7Jo0aIMHDgwyXu3hg4aNCgrVqzI0qVLW6PY6tWrU6lUcuSRR253C+hvf/vbDBkyJKecckoWLFjQepXamjVrcvrpp2f16tV55plncsIJJ+zxiWy7DbVSqbgtFAAAAOAg1l6dqKor1jp27JgpU6Zk69atGTp0aMaOHZvx48dnwIABee655zJp0qTtrjSbMGFC+vXrl4cffni7cf7iL/4iN954Y/73f/83p512Wv72b/82Y8eOzYABA/LKK6/kW9/6VlVRDQAAAADaW8dq33DuuedmwYIFmThxYqZPn5533303/fv3z2233ZZRo0bt8Tjf+c530r9///zwhz/MT3/609TU1OTP//zP8+Mf/zif//znq50WAAAAALSrqm4F3Z+5FRQAAACAZD+9FRQAAAAAeI+wBgAAAAAlCGsAAAAAUIKwBgAAAAAlCGsAAAAAUIKwBgAAAAAlCGsAAAAAUIKwBgAAAAAlCGsAAAAAUIKwBgAAAAAlCGsAAAAAUIKwBgAAAAAlCGsAAAAAUIKwBgAAAAAlCGsAAAAAUIKwBgAAAAAlCGsAAAAAUIKwBgAAAAAlCGsAAAAAUIKwBgAAAAAlCGsAAAAAUIKwBgAAAAAlCGsAAAAAUIKwBgAAAAAlCGsAAAAAUIKwBgAAAAAlCGsAAAAAUIKwBgAAAAAlCGsAAAAAUIKwBgAAAAAlCGsAAAAAUIKwBgAAAAAlCGsAAAAAUIKwBgAAAAAlCGsAAAAAUIKwBgAAAAAlCGsAAAAAUIKwBgAAAAAlCGsAAAAAUIKwBgAAAAAlCGsAAAAAUIKwBgAAAAAlCGsAAAAAUIKwBgAAAAAlCGsAAAAAUIKwBgAAAAAlCGsAAAAAUIKwBgAAAAAlCGsAAAAAUIKwBgAAAAAlCGsAAAAAUIKwBgAAAAAlCGsAAAAAUIKwBgAAAAAlCGsAAAAAUIKwBgAAAAAlCGsAAAAAUIKwBgAAAAAlCGsAAAAAUIKwBgAAAAAlCGsAAAAAUIKwBgAAAAAllAprixcvzsiRI9O9e/fU1dVl8ODBefDBB0tPYt26dTnqqKNSU1OTESNGlB4HAAAAANpLx2rfMHfu3DQ2NqZz584ZPXp0unXrlhkzZmTUqFFZtWpVxo8fX/UkvvSlL6VSqVT9PgAAAAD4sFR1xdrmzZtz9dVXp0OHDvnP//zP/OQnP8l3vvOd/O53v8sJJ5yQm2++OStXrqxqAjNmzMgvfvGL3HbbbVW9DwAAAAA+TFWFtTlz5mT58uW57LLLMnDgwNbtDQ0Nufnmm7Np06ZMmzZtj8dbu3Ztxo0bly9+8Yu58MILq5kKAAAAAHyoqgpr8+bNS5IMHz58h32NjY1Jkvnz5+/xeNdee20OOeSQ3HnnndVMAwAAAAA+dFU9Y23ZsmVJkr59++6wr1evXunatWvrMR/k/vvvz0MPPZRHHnkkPXr0qPoZay0tLWlpaWn9ubm5uar3AwAAAMDeqOqKtW3xq6GhYaf76+vr9yiQvfrqq7n++utz6aWX5nOf+1w1U2g1efLkNDQ0tL6OOeaYUuMAAAAAQBlVhbW2ctVVV+XQQw/N97///dJjTJgwIZVKpfW1atWqNpwhAAAAAOxeVbeCbrtSbVdXpTU3N6dHjx67HWPatGmZOXNmfvnLX+ajH/1oNR+/ndra2tTW1pZ+PwAAAADsjaquWNv2bLWdPUdtzZo12bBhw06fv/Z+Tz31VJLkkksuSU1NTevr+OOPT5I0NTWlpqZmu28dBQAAAID9TVVXrA0bNiyTJ0/O7NmzM3r06O32NTU1tR6zO2eddVY2bNiww/YNGzZk+vTpOfroo9PY2Jhjjz22mqkBAAAAQLuqKYqi2NODN2/enBNPPDGvvPJKFi1a1HpVWaVSyaBBg7JixYosXbo0xx13XJJk9erVqVQqOfLII3f5hQfbrFixIscff3waGxsza9asqk+kubk5DQ0NqVQqqa+vr/r9AAAAABwY2qsTVXUraMeOHTNlypRs3bo1Q4cOzdixYzN+/PgMGDAgzz33XCZNmtQa1ZL3vmCgX79+efjhh9t63gAAAADwoarqVtAkOffcc7NgwYJMnDgx06dPz7vvvpv+/fvntttuy6hRo/bFHAEAAABgv1PVraD7M7eCAgAAAJDsp7eCAgAAAADvEdYAAAAAoARhDQAAAABKENYAAAAAoARhDQAAAABKENYAAAAAoARhDQAAAABKENYAAAAAoARhDQAAAABKENYAAAAAoARhDQAAAABKENYAAAAAoARhDQAAAABKENYAAAAAoARhDQAAAABKENYAAAAAoARhDQAAAABKENYAAAAAoARhDQAAAABKENYAAAAAoARhDQAAAABKENYAAAAAoARhDQAAAABKENYAAAAAoARhDQAAAABKENYAAAAAoARhDQAAAABKENYAAAAAoARhDQAAAABKENYAAAAAoARhDQAAAABKENYAAAAAoARhDQAAAABKENYAAAAAoARhDQAAAABKENYAAAAAoARhDQAAAABKENYAAAAAoARhDQAAAABKENYAAAAAoARhDQAAAABKENYAAAAAoARhDQAAAABKENYAAAAAoARhDQAAAABKENYAAAAAoARhDQAAAABKENYAAAAAoARhDQAAAABKENYAAAAAoARhDQAAAABKENYAAAAAoARhDQAAAABKENYAAAAAoARhDQAAAABKENYAAAAAoARhDQAAAABKENYAAAAAoARhDQAAAABKENYAAAAAoARhDQAAAABKENYAAAAAoIRSYW3x4sUZOXJkunfvnrq6ugwePDgPPvjgHr23KIrMnDkz48aNy2mnnZaGhoYcdthhGTBgQCZNmpSNGzeWmRIAAAAAtKuaoiiKat4wd+7cNDY2pnPnzhk9enS6deuWGTNmZOXKlbnjjjsyfvz43b5/48aN6dKlS2pra3POOeekf//+2bhxY5qamrJs2bKcccYZmTdvXg477LCqTqS5uTkNDQ2pVCqpr6+v6r0AAAAAHDjaqxNVFdY2b96ck046KS+//HIWLVqUgQMHJkkqlUoGDRqUFStW5Lnnnkvv3r13Oca7776b22+/Pdddd1169Oix3faLL744jz76aG6//fbcdNNNVZ2IsAYAAABA0n6dqKpbQefMmZPly5fnsssua41qSdLQ0JCbb745mzZtyrRp03Y7xqGHHpqvfvWr20W1bdsnTJiQJJk/f3410wIAAACAdldVWJs3b16SZPjw4Tvsa2xsTLJ3UezQQw9NknTs2LH0GAAAAADQHqoqWMuWLUuS9O3bd4d9vXr1SteuXVuPKeO+++5LsvNw96daWlrS0tLS+nNzc3PpzwUAAACAalV1xVqlUkny3q2fO1NfX996TLVmzpyZe+65J/369cuVV175gcdPnjw5DQ0Nra9jjjmm1OcCAAAAQBlVhbV9ZfHixRk1alQaGhryy1/+MrW1tR/4ngkTJqRSqbS+Vq1a1Q4zBQAAAID3VHUr6LYr1XZ1VVpzc/MOX0rwQZYsWZLhw4enQ4cOaWpqyimnnLJH76utrd2jAAcAAAAA+0JVV6xte7bazp6jtmbNmmzYsGGnz1/blSVLluSCCy7I1q1b09TUlDPOOKOa6QAAAADAh6aqsDZs2LAkyezZs3fY19TUtN0xH2RbVNuyZUtmzZqVM888s5qpAAAAAMCHqqYoimJPD968eXNOPPHEvPLKK1m0aFEGDhyY5L1bQwcNGpQVK1Zk6dKlOe6445Ikq1evTqVSyZFHHrndFx7893//d84///xs3rw5s2bNypAhQ/b6RJqbm9PQ0JBKpZL6+vq9Hg8AAACA/5vaqxNVFdaSZO7cuWlsbEznzp0zevTodOvWLTNmzMjKlStzxx13ZPz48a3HjhkzJtOmTcvUqVMzZsyYJMmbb76ZP/uzP8u6desyYsSInV6p1r179/z93/99VScirAEAAACQtF8nqurLC5Lk3HPPzYIFCzJx4sRMnz497777bvr375/bbrsto0aN+sD3Nzc3Z926dUmSWbNmZdasWTsc07t376rDGgAAAAC0p6qvWNtfuWINAAAAgKT9OlFVX14AAAAAALxHWAMAAACAEoQ1AAAAAChBWAMAAACAEoQ1AAAAAChBWAMAAACAEoQ1AAAAAChBWAMAAACAEoQ1AAAAAChBWAMAAACAEoQ1AAAAAChBWAMAAACAEoQ1AAAAAChBWAMAAACAEoQ1AAAAAChBWAMAAACAEoQ1AAAAAChBWAMAAACAEoQ1AAAAAChBWAMAAACAEoQ1AAAAAChBWAMAAACAEoQ1AAAAAChBWAMAAACAEoQ1AAAAAChBWAMAAACAEoQ1AAAAAChBWAMAAACAEoQ1AAAAAChBWAMAAACAEoQ1AAAAAChBWAMAAACAEoQ1AAAAAChBWAMAAACAEoQ1AAAAAChBWAMAAACAEoQ1AAAAAChBWAMAAACAEoQ1AAAAAChBWAMAAACAEoQ1AAAAAChBWAMAAACAEoQ1AAAAAChBWAMAAACAEoQ1AAAAAChBWAMAAACAEoQ1AAAAAChBWAMAAACAEoQ1AAAAAChBWAMAAACAEoQ1AAAAAChBWAMAAACAEoQ1AAAAAChBWAMAAACAEoQ1AAAAAChBWAMAAACAEoQ1AAAAAChBWAMAAACAEoQ1AAAAAChBWAMAAACAEoQ1AAAAAChBWAMAAACAEkqFtcWLF2fkyJHp3r176urqMnjw4Dz44INVjdHS0pJbb701ffv2TefOnfPxj388Y8eOzWuvvVZmSgAAAADQrjpW+4a5c+emsbExnTt3zujRo9OtW7fMmDEjo0aNyqpVqzJ+/PgPHGPr1q353Oc+l6ampgwePDgXX3xxli1blilTpuQ//uM/smjRovTs2bPUCQEAAABAe6gpiqLY04M3b96ck046KS+//HIWLVqUgQMHJkkqlUoGDRqUFStW5Lnnnkvv3r13O87UqVPzN3/zN7n00kvz85//PDU1NUmSH//4xxk3blzGjh2be+65p6oTaW5uTkNDQyqVSurr66t6LwAAAAAHjvbqRFXdCjpnzpwsX748l112WWtUS5KGhobcfPPN2bRpU6ZNm/aB49x7771JksmTJ7dGtSS55ppr0qdPn/z85z/PO++8U83UAAAAAKBdVRXW5s2blyQZPnz4DvsaGxuTJPPnz9/tGBs3bsyTTz6ZE088cYcr22pqanLBBRfk7bffzpIlS6qZGgAAAAC0q6qesbZs2bIkSd++fXfY16tXr3Tt2rX1mF1Zvnx5tm7dutMx3j/2smXL8qlPfWqX47S0tKSlpaX150qlkuS9S/0AAAAAOHht60NVPAGtlKrC2rZ41dDQsNP99fX1rcfszRjvP25XJk+enFtuuWWH7cccc8xu3wcAAADAweGNN97YZYNqC1V/K+j+YsKECbnxxhtbf16/fn169+6dl156aZ/+BwOq19zcnGOOOSarVq3y5SKwH7JGYf9lfcL+zRqF/VelUsmxxx6bj3zkI/v0c6oKa9uC1a6uJmtubk6PHj32eoz3H7crtbW1qa2t3en4/gcN9k/19fXWJ+zHrFHYf1mfsH+zRmH/1aFDVV8vUP341Rz8/uef/ak1a9Zkw4YNu3x22jZ9+vRJhw4ddvkstt09xw0AAAAA9hdVhbVhw4YlSWbPnr3Dvqampu2O2ZUuXbpk0KBBWbp0aVauXLndvqIo8vjjj6euri6nn356NVMDAAAAgHZVVVg777zz0qdPn/ziF7/I008/3bq9Uqlk0qRJ6dSpUy6//PLW7atXr86zzz67w22fY8eOTfLec9Le/+0M99xzT1544YV84QtfSJcuXao6kdra2kycOHGnt4cCHy7rE/Zv1ijsv6xP2L9Zo7D/aq/1WVNU+b2jc+fOTWNjYzp37pzRo0enW7dumTFjRlauXJk77rgj48ePbz12zJgxmTZtWqZOnZoxY8a0bt+6dWtGjhyZpqamDB48OMOGDcvzzz+fhx56KMcdd1yefPLJ9OzZs81OEgAAAADaWtVPcDv33HOzYMGCDBkyJNOnT8/dd9+dI444Ig888MB2UW23H9qhQ37961/nG9/4RtauXZvvfve7WbhwYa688so88cQTohoAAAAA+72qr1gDAAAAAEpcsQYAAAAACGsAAAAAUMp+G9YWL16ckSNHpnv37qmrq8vgwYPz4IMPVjVGS0tLbr311vTt2zedO3fOxz/+8YwdOzavvfbaPpo1HDz2Zo0WRZGZM2dm3LhxOe2009LQ0JDDDjssAwYMyKRJk7Jx48Z9PHs4sLXF79D3W7duXY466qjU1NRkxIgRbThTODi11Rp97bXXcsMNN7T+rXv44YfnrLPOyt13370PZg0Hh7ZYn6+++mq+/OUv5+STT05dXV2OOOKInH322fnZz36WLVu27KOZw4Hv/vvvzzXXXJPTTz89tbW1qampyU9/+tOqx9m6dWvuuuuu9O/fP126dEnPnj1z6aWX5oUXXig1r/3yGWvVfPPoruzsm0eXLVuWhx9+OMcff3wWLVrkSxKgpL1doxs3bkyXLl1SW1ubc845J/3798/GjRvT1NSUZcuW5Ywzzsi8efNy2GGHtdMZwYGjLX6H/qkvfOEL+fWvf5233347jY2NmTVr1j6YORwc2mqNPv300xk+fHjWrVuXCy+8MP369cuGDRvyzDPPpFOnTnnsscf28ZnAgact1ucLL7yQM888M2+88UYaGxtz2mmnpbm5OY888kjWrFmTMWPGZOrUqe1wNnDgOe6447Jy5cp89KMfTV1dXVauXJmpU6dmzJgxVY1z9dVXZ8qUKTnllFNy4YUX5tVXX82DDz6Yrl27ZtGiRenbt291Eyv2M++++27xiU98oqitrS2eeuqp1u3r168vTjjhhKJTp07FihUrPnCc++67r0hSXHrppcXWrVtbt999991FkmLs2LH7YvpwwGuLNbpp06biW9/6VvHmm2/usP2iiy4qkhS33377vpg+HNDa6nfo+/3qV78qkhQ/+MEPiiRFY2NjG88aDh5ttUYrlUpx7LHHFj179ix+97vf7fRzgOq01focN25ckaT43ve+t932devWFccee2yRpOrfxcB7Hn/88db1M3ny5CJJMXXq1KrGmDNnTpGkGDp0aNHS0tK6/bHHHiuSFMOHD696XvvdraBz5szJ8uXLc9lll2XgwIGt2xsaGnLzzTdn06ZNmTZt2geOc++99yZJJk+enJqamtbt11xzTfr06ZOf//zneeedd9p8/nCga4s1euihh+arX/1qevToscP2CRMmJEnmz5/f5nOHA11b/Q7dZu3atRk3bly++MUv5sILL9wHM4aDS1ut0R/96Ed56aWX8u1vfzunnXbaDvs7duzYltOGg0Jbrc9tt5KNHDlyu+3du3fP2WefnSR5/fXX227icBA5//zz07t3770aY1sr+uY3v5lOnTq1bv/MZz6Tc845J7Nnz85LL71U1Zj7XVibN29ekmT48OE77GtsbEzywf/g3rhxY5588smceOKJO/xHr6mpyQUXXJC33347S5YsaZtJw0GkLdbo7hx66KFJ/KMAymjr9XnttdfmkEMOyZ133tkm84ODXVut0enTp6empiYXX3xxli5dmrvuuiu33357/u3f/i2bNm1q0znDwaKt1uepp56aJDvcjr1+/fosXLgwvXr1ysknn7yXswXKmjdvXurq6jJkyJAd9pX99+x+9y/XZcuWJclO72nt1atXunbt2nrMrixfvjxbt27d5X2x27YvW7Ysn/rUp/ZyxnBwaYs1ujv33Xdfkp3/UQPsXluuz/vvvz8PPfRQHnnkkfTo0SOVSqVN5woHo7ZYo5s2bcrvf//79OzZM3fddVcmTpyYrVu3tu7v06dPHnnkkfTv379tJw8HuLb6HXrTTTfl0UcfzQ033JBZs2Zt94y1ww47LA8//HC6dOnS5vMHPtjbb7+d1atX59RTT80hhxyyw/73t6Jq7HdXrG37w72hoWGn++vr6z/wj/s9GeP9xwF7ri3W6K7MnDkz99xzT/r165crr7yy9BzhYNVW6/PVV1/N9ddfn0svvTSf+9zn2nSOcDBrizX65ptvZsuWLXnjjTdy66235vbbb88f//jHvPzyy/na176WF198MRdddJFv2IYqtdXv0COOOCJPPPFERowYkVmzZuX222/Pj3/841QqlVx++eUZMGBAm84b2HP7qhXtd2ENODgtXrw4o0aNSkNDQ375y1+mtrb2w54SHLSuuuqqHHroofn+97//YU8F+BPbrk7bsmVLrrvuuowfPz4f+9jHctRRR+XWW2/NJZdckpUrV+ZXv/rVhzxTODg9//zzGTJkSNauXZvf/OY3eeutt7Jq1ap8/etfzze/+c2cd9552bJly4c9TaAN7XdhbVs53FUhbG5u3mVdrGaM9x8H7Lm2WKN/asmSJRk+fHg6dOiQpqamnHLKKXs9TzgYtcX6nDZtWmbOnJkf/vCH+ehHP9rmc4SDWVv+nZskn/3sZ3fYv22bZwlDddrqb9wxY8Zk5cqVefTRR3P22Wena9euOfroo/OVr3wlf/d3f5cnnngiDzzwQJvOHdgz+6oV7XdhbXf3tK5ZsyYbNmzY5bPTtunTp086dOiwy/tid3f/PLB7bbFG32/JkiW54IILsnXr1jQ1NeWMM85os7nCwaYt1udTTz2VJLnkkktSU1PT+jr++OOTJE1NTampqdnuG9OAPdMWa7Suri5HHXVUkve+ZfBPbdv2zjvv7N1k4SDTFuvzrbfeysKFC9OvX7/06tVrh/3nnntukv//uxZoX3V1dTnyyCPz4osv7vTK0bKtaL8La8OGDUuSzJ49e4d9TU1N2x2zK126dMmgQYOydOnSrFy5crt9RVHk8ccfT11dXU4//fQ2mjUcPNpijW6zLapt2bIls2bNyplnntl2E4WDUFusz7POOitXXnnlDq9Ro0YlSY4++uhceeWV+cu//Ms2nj0c+Nrqd+inP/3pJMkf/vCHHfZt23bccceVnSYclNpifW77Vt7XX399p/vXrl2bJB55Ah+iYcOG5e23387ChQt32LdtrQ8dOrS6QYv9zLvvvlv06dOnqK2tLZ566qnW7evXry9OOOGEolOnTsWLL77Yuv3VV18tnnnmmWL9+vXbjXPfffcVSYpLL7202Lp1a+v2u+++u0hSjB07dl+fChyQ2mqNLlmypOjevXvRtWvXYsGCBe00eziwtdX63JkXX3yxSFI0Njbug5nDwaGt1ujChQuLJMUpp5xSrFu3rnX76tWri6OOOqro0KFDsXTp0n18NnBgaav1eeKJJxZJinvvvXe77evWrStOOumkIknx+OOP78tTgYPC5MmTiyTF1KlTd7p/7dq1xTPPPFOsXbt2u+1z5swpkhRDhw4tWlpaWrc/9thjRZJi+PDhVc9lvwtrRfHeiR566KFFt27diquvvrq48cYbi969exdJijvuuGO7Y6+44oqd/sfcsmVL0djYWCQpBg8eXPzTP/1TcfHFFxc1NTXF8ccfX7z22mvteEZwYNnbNfrGG28UPXr0KJIUI0aMKCZOnLjD67vf/W77nhQcINrid+jOCGvQNtpqjd54441FkuKYY44prrvuuuLqq68uPvaxjxVJikmTJrXT2cCBpS3W52OPPVZ07NixSFKcd955xT/8wz8UV155ZdGzZ88iSXHxxRe34xnBgeXee+8trrjiiuKKK64oPvnJTxZJiiFDhrRue3/QnjhxYpGkmDhx4g7jXHXVVa3/B9U//uM/Fl/84heLTp06FR/5yEdK/R9T+2VYK4qiePLJJ4sRI0YU9fX1RZcuXYpBgwYVDzzwwA7H7e4Pjo0bNxbf+MY3ik984hNFp06dil69ehVXXXVVsWbNmnY4Aziw7c0a3fYP9N29evfu3X4nAweYtvgd+qeENWg7bbVGp06dWpx++unFYYcdVtTV1RVnn3128dBDD+3j2cOBrS3W53/9138Vl1xySXHkkUcWHTt2LLp27VqcccYZxV133VVs3ry5Hc4CDkzb1t2uXldccUXrsbsLa1u2bCnuvPPO4pRTTilqa2uLww8/vBg1alTx/PPPl5pXTVEURXU3jwIAAAAA+92XFwAAAADA/wXCGgAAAACUIKwBAAAAQAnCGgAAAACUIKwBAAAAQAnCGgAAAACUIKwBAAAAQAnCGgAAAACUIKwBAAAAQAnCGgAAAACUIKwBAAAAQAnCGgAAAACUIKwBAAAAQAn/D6Y/p9NGdwh4AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for res in  results:\n",
    "    plot_train_v_loss(f\"team-as-entity-{res[0][0]}-{res[0][1]}\", res[1][1], res[1][2], loss_interval)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
